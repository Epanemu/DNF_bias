Command:
python test_script.py -s linear_dependence -d 6 -n 500 --seed 21 -m dnf_mio -k 6 --verbose
Output:
The true theoretical sup(\mu - \nu) = 0.15000000000000002
The correct rule on sampled data has \hat{\mu} - \hat{\nu} = 0.14400000000000002
TRIVIAL ACCURACY - always TRUE: 0.5
Balancing dropped 0 samples, 500 remain. 
Dimension is 6.

Computed total variation: 0.28400000000000003
DNF using MIO
Set parameter TimeLimit to value 120
Gurobi Optimizer version 11.0.3 build v11.0.3rc0 (win64 - Windows 11.0 (22631.2))

CPU model: Intel(R) Core(TM) i9-14900HX, instruction set [SSE2|AVX|AVX2]
Thread count: 24 physical cores, 32 logical processors, using up to 32 threads

Optimize a model with 16500 rows, 1810 columns and 32750 nonzeros
Model fingerprint: 0x256e583e
Variable types: 1750 continuous, 60 integer (60 binary)
Coefficient statistics:
  Matrix range     [1e+00, 1e+00]
  Objective range  [4e-03, 4e-03]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 4e+00]
Found heuristic solution: objective 1.0000000
Presolve removed 14268 rows and 1316 columns
Presolve time: 0.01s
Presolved: 2232 rows, 494 columns, 6262 nonzeros
Variable types: 434 continuous, 60 integer (60 binary)

Root relaxation: objective 0.000000e+00, 491 iterations, 0.00 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0    0.00000    0   16    1.00000    0.00000   100%     -    0s
H    0     0                       0.9240000    0.00000   100%     -    0s
H    0     0                       0.8760000    0.00000   100%     -    0s
     0     0    0.00000    0   21    0.87600    0.00000   100%     -    0s
H    0     0                       0.8160000    0.00000   100%     -    0s
     0     0    0.00000    0    8    0.81600    0.00000   100%     -    0s
     0     0    0.00000    0    8    0.81600    0.00000   100%     -    0s
     0     0    0.00000    0   10    0.81600    0.00000   100%     -    0s
     0     0    0.00000    0   14    0.81600    0.00000   100%     -    0s
     0     0    0.00000    0   10    0.81600    0.00000   100%     -    0s
     0     0    0.00000    0   13    0.81600    0.00000   100%     -    0s
     0     0    0.00000    0    8    0.81600    0.00000   100%     -    0s
     0     0    0.00000    0   30    0.81600    0.00000   100%     -    0s
     0     0    0.00000    0   10    0.81600    0.00000   100%     -    0s
     0     0    0.00000    0   10    0.81600    0.00000   100%     -    0s
     0     2    0.00000    0    9    0.81600    0.00000   100%     -    0s
H  106   128                       0.7920000    0.00000   100%   147    0s
H  107   128                       0.7880000    0.00000   100%   147    0s
H  224   249                       0.7760000    0.00000   100%   124    1s
H  239   249                       0.7680000    0.00000   100%   126    1s
H  389   413                       0.7400000    0.00000   100%   105    2s
H 1526  1330                       0.7360000    0.00000   100%  61.6    3s
  3173  2383    0.61867   33   18    0.73600    0.00000   100%  51.5    5s
  3329  2523    0.14338   47   33    0.73600    0.14212  80.7%  63.9   10s
H 3423  2431                       0.7320000    0.15218  79.2%  66.3   10s
  9398  4898    0.40393   55   15    0.73200    0.17138  76.6%  57.9   15s
 16862  9076    0.39231   61   27    0.73200    0.20710  71.7%  51.9   20s
 26015 15474    0.30523   56   27    0.73200    0.23595  67.8%  49.4   25s
 37661 22775    0.40720   56   31    0.73200    0.25666  64.9%  46.8   30s
 49792 29852    0.65700   69   14    0.73200    0.27214  62.8%  44.3   35s
 60455 34922    0.50833   62   10    0.73200    0.28665  60.8%  42.9   43s
 60464 34928    0.33095   54   57    0.73200    0.28665  60.8%  42.9   45s
 60585 35042    0.28665   62   30    0.73200    0.28665  60.8%  43.0   50s
 64278 36066    0.36348   66   18    0.73200    0.28665  60.8%  42.9   55s
 69885 37019    0.44100   69   17    0.73200    0.34018  53.5%  42.2   60s
 75802 38023    0.47155   66   27    0.73200    0.36698  49.9%  41.6   65s
 82340 38522    0.73040   80    9    0.73200    0.38288  47.7%  40.7   70s
 90217 39738    0.62624   75   23    0.73200    0.40001  45.4%  39.9   75s
 99615 40846    0.72640   97    8    0.73200    0.41327  43.5%  38.9   80s
 110952 42333    0.44212   71   28    0.73200    0.42607  41.8%  37.8   85s
 122908 43907    0.64444   75   15    0.73200    0.43624  40.4%  37.0   91s
 132016 44698    0.69040   75   15    0.73200    0.44616  39.0%  36.4   95s
 145067 46101    0.50318   73   19    0.73200    0.45724  37.5%  35.6  101s
 156138 47293    0.71770   83   24    0.73200    0.46431  36.6%  34.9  106s
 166757 48595    0.60173   76   22    0.73200    0.47121  35.6%  34.4  111s
 177256 52835    0.52767   74   17    0.73200    0.47665  34.9%  33.9  116s
 187884 55829    0.71504   92   13    0.73200    0.48093  34.3%  33.4  120s

Cutting planes:
  Gomory: 97
  MIR: 152
  Flow cover: 1737
  RLT: 42
  Relax-and-lift: 304

Explored 189022 nodes (6309705 simplex iterations) in 120.21 seconds (42.61 work units)
Thread count was 32 (of 32 available processors)

Solution count 10: 0.732 0.736 0.74 ... 0.924

Time limit reached
Best objective 7.320000000000e-01, best bound 4.816494047619e-01, gap 34.2009%
WARNING: Loading a SolverResults object with an 'aborted' status, but
containing a solution
FULL MODEL:
  Accruacy: 0.53
  Our objective: 0.06000000000000004

IF 
    (x0 = 0 AND x1 = 0)                       <-- (term's our objective: 0.184)
 OR (x0 = 0 AND x1 = 1 AND x3 = 1 AND x4 = 0) <-- (term's our objective: 0.02)
 OR (x0 = 1 AND x1 = 1 AND x2 = 0 AND x4 = 1) <-- (term's our objective: 0.016)
 OR (x1 = 0 AND x4 = 1 AND x5 = 1)            <-- (term's our objective: 0.068)
 OR (x3 = 1 AND x4 = 0 AND x5 = 1)            <-- (term's our objective: 0.06)
THEN
 target = 1.0 ELSE target = 0.0

Seconds needed: 121.2606291770935
Best over terms:
  Our final objective: 0.18399999999999997
    Its accruacy: 0.592
    Its hamming distance: 1
  Shortest hamming distance: 1
    Its our objective: 0.18399999999999997
  Highest accruacy: 0.592

Errors:

